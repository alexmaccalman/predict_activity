---
title: "predict_activity"
author: "Alex MacCalman"
date: "9/8/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#set up parallel processing
doParallel::registerDoParallel()
```
This prohect develop a prediction model to predict whether a person lifts a barbell correctly or incorrectly in 5 different ways. More information is available from the website [here:](http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset)).


# Load and explore the data.  
```{r}
library(tidyverse)
library(dplyr)
library(tidymodels)
library(GGally)
library(skimr)
library(corrr)
library(naniar)
library(glmnet)
library(tidypredict)
library(yaml)

data_raw <- read_csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
valid_raw <- read_csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
#explore data variables
skim((data_raw))
#explore the missing value impacts
#select missing data
missing_data <- data_raw %>%
  select_if(~ any(is.na(.)))

missing_data %>% 
  gg_miss_upset()

# eliminate the variables with the missing data in training data
data_clean <- data_raw %>%
  select_if(~ !any(is.na(.))) %>% 
  select(-cvtd_timestamp, -new_window, -X1, -raw_timestamp_part_1, -raw_timestamp_part_2, -num_window)

data_clean$classe <- as.factor(data_clean$classe)

# eliminate the variables with the missing data in testing data
valid_clean <- valid_raw %>%
  select_if(~ !any(is.na(.))) %>% 
  select(-user_name, -cvtd_timestamp, -new_window, -X1, -raw_timestamp_part_1, -raw_timestamp_part_2, -num_window, -problem_id)


# narrow in on the reduced data set
skim(data_clean)
#train <- as_tibble(train)
#glimpse(train)

data_clean %>% 
  count(classe) %>% 
  mutate(prop = n/sum(n))

##Explore the numeric correlations. 
all_numeric <- select_if(data_clean, is.numeric)

cor_threshold <- 0.8
correlate(all_numeric) %>% 
  stretch() %>% 
  mutate(threshold = case_when(r > cor_threshold | r < -cor_threshold ~ 1,
                          TRUE ~ 0)) %>%
  count(threshold)

cor <- correlate(all_numeric) %>% 
  stretch() %>% 
  filter(r > cor_threshold | r < -cor_threshold, r != 1) %>% 
  arrange(desc(r))

high_cor <- all_numeric %>% 
  select(as.character(cor$x))

high_cor %>% 
  correlate() %>% 
  rearrange(method = "MDS", absolute = FALSE) %>% 
  shave() %>% 
  rplot(shape = 19, colors = c("red", "green"))

# explore k-means clusters 
# select variables we want to cluster on
points <- data_clean %>% 
  select(-classe, -user_name)

#explore the number of clusters
kclusts <- 
  tibble(k = 1:20) %>%
  mutate(
    kclust = map(k, ~kmeans(points, .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, points)
  )
# assign separate lists
clusters <- 
  kclusts %>%
  unnest(cols = c(tidied))

assignments <- 
  kclusts %>% 
  unnest(cols = c(augmented))

clusterings <- 
  kclusts %>%
  unnest(cols = c(glanced))

#find best number of clusters
ggplot(clusterings, aes(k, tot.withinss)) +
  geom_line() +
  geom_point()
```
# Split the training and testing sets and set up cross validation
```{r}
set.seed(123)
data_split <- initial_split(data_clean, strata = classe)
train <- training(data_split)
test <- testing(data_split)
# create the cross validation sets
vfolds <- vfold_cv(train, v = 3, strata = classe) # 3 folds

```



# Fit reg model without pca
```{r}
# build recipe without PCA  
reg_recipe <- 
  recipe(classe ~ ., data = train) %>% 
  update_role(user_name, new_role = "person_name") %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% # outcome is a factor and don't want dummies
  step_normalize(all_predictors()) 

prepped <- prep(reg_recipe) %>% juice()

# Build multi-nominal regularization specification
reg_spec <- multinom_reg(penalty = 0.1, mixture = 0.5) %>% 
  set_engine("glmnet") %>% 
  set_mode("classification")

# Build workflow for recipe without PCA 
reg_wf <- 
  workflow() %>% 
  add_model(reg_spec) %>% 
  add_recipe(reg_recipe)

```

## Fit a single model without PCA and evaluate performance
```{r, cache = TRUE}
#fit on 3 folds
reg_result <- fit_resamples(
  reg_wf,
  resamples = vfolds,
  control = control_resamples(save_pred = TRUE)
)
# explore the results
reg_result %>% 
  collect_metrics()
# create a confusion matrix
reg_result2 %>% 
  collect_predictions() %>% 
  conf_mat(classe, .pred_class)
```


# Fit reg model with PCA
```{r}
# build recipe with PCA 
pca_fit <- train %>% 
  select(where(is.numeric)) %>% # retain only numeric columns
  scale() %>% # scale the data
  prcomp() # do PCA
           
# create a component table
pca_fit %>%
  tidy(matrix = "eigenvalues")
# display the components
pca_fit %>%
  tidy(matrix = "eigenvalues") %>%
  ggplot(aes(PC, percent)) +
  geom_col(fill = "#56B4E9", alpha = 0.8) +
  scale_x_continuous(breaks = 1:56) +
  scale_y_continuous(
    labels = scales::percent_format(),
    expand = expansion(mult = c(0, 0.01))
  )# we will use the first 25 components to build the recipe

#build a recipe with PCA
reg_pca_recipe <- 
  recipe(classe ~ ., data = train) %>% 
  update_role(user_name, new_role = "person_name") %>%
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_pca(all_numeric(), threshold = 0.80) # take the pcas that explain 80% of variation

# Build workflow for recipe with PCA  
reg_pca_wf <- 
  workflow() %>% 
  add_model(reg_spec) %>% 
  add_recipe(reg_pca_recipe)
```

## Fit and tune Multi-nomial regularized model with PCA 
```{r, cache = TRUE}

#fit on 3 folds
reg_pca_result <- fit_resamples(
  reg_pca_wf,
  resamples = vfolds,
  control = control_resamples(save_pred = TRUE)
)
# explore the results
reg_pca_result %>% 
  collect_metrics()
# create a confusion matrix
reg_pca_result %>% 
  collect_predictions() %>% 
  conf_mat(classe, .pred_class)
```

# Fit a xgboost model
```{r}
# Build xgboost specification
xgb_spec <- boost_tree() %>% 
        set_engine("xgboost") %>% 
        set_mode("classification")
# Build the xgboost workflow
# we don't need a recipe. We will use all the data
xgb_wf <- workflow() %>% 
        add_formula(classe ~ .) %>% 
        add_model(xgb_spec)
```
 
## Fit the xgboost model
```{r, cache = TRUE}
set.seed(234)

#fit on 3 folds
xgb_result <- fit_resamples(
  xgb_wf,
  resamples = vfolds,
  control = control_resamples(save_pred = TRUE)
)
# explore the results
xgb_result %>% 
  collect_metrics()
# create a confusion matrix
xgb_result %>% 
  collect_predictions() %>% 
  conf_mat(classe, .pred_class)
```

# Tune the xgboost model
```{r}
#make model specification
xgb_tune_spec <- boost_tree(
        trees = 1000,
        tree_depth = tune(), min_n = tune(), loss_reduction = tune(),
        sample_size = tune(), mtry = tune(),
        learn_rate = tune()) %>% 
        set_engine("xgboost") %>% 
        set_mode("classification")

#set up what values we will try
xgb_grid <- grid_latin_hypercube(
        tree_depth(),
        min_n(),
        loss_reduction(),
        sample_size = sample_prop(), 
        finalize(mtry(), train),
        learn_rate(),
        size = 10
)

#set up a workflow
xgb_tune_wf <- workflow() %>% 
        add_formula(classe ~ .) %>% 
        add_model(xgb_tune_spec)

```
## Fit and tune the xgboost model  
```{r, cache=TRUE}
set.seed(234)
starttime <- Sys.time()
xbg_tune_res<- tune_grid(
        xgb_tune_wf,
        resamples = vfolds,
        grid = xgb_grid,
        control = control_grid(save_pred = TRUE)
)
endtime <- Sys.time()
tottime <- endtime - starttime
```
## Explore the results.  
```{r}
# make a visualization
xbg_tune_res %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry:sample_size,
               values_to = "value",
               names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "AUC")
    
```
## find the best
```{r}
show_best(xbg_tune_res, "roc_auc")

best_auc <- select_best(xbg_tune_res, "roc_auc")

# finalize the workflow
final_xgb_wf <- finalize_workflow(xgb_tune_wf, best_auc)

```

## pull out the final model
```{r, cache=TRUE}
# Fit the final best model to the training set and evaluate the test set
final_xgb_results <- last_fit(final_xgb, data_split)
#explore results
final_xgb_results %>% 
        collect_metrics()
# create confusion matrix
final_xgb_results %>% 
        collect_predictions() %>% 
        conf_mat(classe, .pred_class)

final_model <- final_xgb_wf %>% 
        fit(data = (train %>% select(-user_name))) %>% 
        pull_workflow_fit()

# extract components needed to make predictions
parsed <- parse_model(final_model)

# display the components of the model
#tidypredict_fit(parsed)

# save the model to a yaml file
write_yaml(parsed, "project8model.yml")

```
# Test Predictions  
```{r}
predictions <- 
  predict(final_model, new_data = valid_clean)
```

